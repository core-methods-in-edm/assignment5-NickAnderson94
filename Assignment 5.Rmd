---
title: "Assignment 5 - Decision Trees"
author: "Charles Lang"
date: "November 9, 2016"
output: html_document
---
For this assignment we will be using data from the Assistments Intelligent Tutoring system. This system gives students hints based on how they perform on math problems. 

#Install & call libraries
```{r Get packages to create decision trees}

#install.packages("party", "rpart") this code is not working for me on either machine
library(rpart)
library(party)
library(ggplot2)
library(knitr)

```

## Part I
```{r setup}

D1 <- read.csv("intelligent_tutor.csv", header = TRUE)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)

```

##Classification Tree
First we will build a classification tree to predict which students ask a teacher for help, which start a new session, or which give up, based on whether or not the student completed a session (D1$complete) and whether or not they asked for hints (D1$hint.y). 
```{r Classification Tree}

c.tree <- rpart(action ~ hint.y + complete, method="class", data=D1) #Notice the standard R notion for a formula X ~ Y

#Look at the error of this tree
printcp(c.tree)

#Plot the tree
post(c.tree, file = "tree.ps", title = "Session Completion Action: 1 - Ask teacher, 2 - Start new session, 3 - Give up")

```
## Part II

#Regression Tree

We want to see if we can build a decision tree to help teachers decide which students to follow up with, based on students' performance in Assistments. We will create three groups ("teacher should intervene", "teacher should monitor student progress" and "no action") based on students' previous use of the system and how many hints they use. To do this we will be building a decision tree using the "party" package. The party package builds decision trees based on a set of statistical stopping rules.

#Take a look at our outcome variable "score"
```{r DV distribution}

hist(D1$score)

```

#Create a categorical outcome variable based on student score to advise the teacher using an "ifelse" statement
```{r Binning continuous DV}

D1$advice <- ifelse(D1$score <=0.4, "intervene", ifelse(D1$score > 0.4 & D1$score <=0.8, "monitor", "no action"))
#aren't we loosing information here?

```

#Build a decision tree that predicts "advice" based on how many problems students have answered before, the percentage of those problems they got correct and how many hints they required
```{r Decision Tree #2}

score_ctree <- ctree(factor(advice) ~ prior_prob_count + prior_percent_correct + hints, D1)

```

#Plot tree
```{r Plot Tree}

plot(score_ctree)
ggsave("Decision Tree.pdf", path = file.path(getwd(), "Images"))

```

Please interpret the tree, which two behaviors do you think the teacher should most closely pay attemtion to?
###the teacher should pay the most attention to their prior percent correct and hints

#Test Tree
Upload the data "intelligent_tutor_new.csv". This is a data set of a differnt sample of students doing the same problems in the same system. We can use the tree we built for the previous data set to try to predict the "advice" we should give the teacher about these new students. 

```{r Import new test dataset}
#Upload new data
D2 <- read.csv("intelligent_tutor_new.csv")

#Generate predicted advice for new students based on tree generated from old students
D2$prediction <- predict(score_ctree, D2)

``` 
## Part III
Compare the predicted advice with the actual advice that these studnts recieved. What is the difference between the observed and predicted results?

```{r Error in model}

D2$advice <- ifelse(D2$score <=0.4, "intervene", ifelse(D2$score > 0.4 & D2$score <=0.8, "monitor", "no action"))

#recode into numeric variables to calculate error
D2$prediction1 <- ifelse(D2$prediction == "intervene", 1, ifelse(D2$prediction == "monitor", 2, 3))
D2$advice1 <- ifelse(D2$advice == "intervene", 1, ifelse(D2$advice == "monitor", 2, 3))

#find difference
sum(abs(D2$advice1 - D2$prediction1))

#the sum of the absolute value is 84, which isnt that bad since there were only 361 observations

#percentage of errors = 23%
84/361

```
